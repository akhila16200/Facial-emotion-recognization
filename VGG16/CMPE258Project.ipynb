{"cells":[{"cell_type":"markdown","metadata":{"id":"mZC3vrRo-YLW"},"source":["# VGG16 finetuned with image augmentation\n","## author: Jiajun Dai\n","### First model: VGG16"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2245,"status":"ok","timestamp":1702515948630,"user":{"displayName":"JiaJun Dai","userId":"04536053890545897660"},"user_tz":480},"id":"59BPIZkNxQSm","outputId":"6f8b1aad-8841-49de-9441-05c4920df064"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# connect with google drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zP_sDC4lDLqd"},"outputs":[],"source":["# !pip install opencv-python"]},{"cell_type":"markdown","metadata":{"id":"DFhFPY5O_wbf"},"source":["Convertor to resize images to 48 x 48 (only run this once)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XfHigbJ43DOy"},"outputs":[],"source":["# from PIL import Image\n","# import os\n","# import cv2\n","\n","# # List files in the folder\n","# files = os.listdir(data_folder_path)\n","# print(files)\n","\n","# # Create a new folder to save the results\n","# save_path = 'content/drive/MyDrive/data/processed_data'\n","# os.makedirs(save_path, exist_ok=True)\n","\n","# for emo_folder in files:\n","#     emo_folder_path = os.path.join(data_folder_path, emo_folder)\n","#     for img_file in os.listdir(emo_folder_path):\n","#       img_path = os.path.join(emo_folder_path, img_file)\n","#       # Read the image\n","#       img = cv2.imread(img_path)\n","#       # Resize the image to 48 x 48 pixels\n","#       img_resized = cv2.resize(img, (48, 48))\n","#       # Convert the image to grayscale\n","#       img_gray = cv2.cvtColor(img_resized, cv2.COLOR_BGR2GRAY)\n","#       # Save the result\n","#       output_path = os.path.join(emo_folder_path, img_file)\n","#       cv2.imwrite(output_path, img_gray)\n","#       print(f\"created processed_{img_file} in {output_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BLz4R98w1-mZ"},"outputs":[],"source":["# !pip install tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DniEe7hWlEdS"},"outputs":[],"source":["import os\n","\n","# Set the paths\n","input_data_path = '/content/drive/Shareddrives/CMPE 258 - Deep Learning Project/Software Programs/Datasets/train_data_processed'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wizH5Ibu3Y3F"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sn\n","import skimage.io\n","import keras.backend as K\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.applications import VGG16\n","from tensorflow.keras.layers import Dense, Flatten, Dropout,BatchNormalization ,Activation\n","from tensorflow.keras.models import Model, Sequential\n","from keras.applications.nasnet import NASNetLarge\n","from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n","from tensorflow.keras.optimizers import Adam"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2gqgODs8wVOp"},"outputs":[],"source":["image_generator = ImageDataGenerator(\n","    # This process is also called Normalizing the input.\n","    # Scaling every images to the same range [0,1] will make images contributes more evenly to the total loss\n","    rescale = 1./255,\n","    # set validation set scale\n","    validation_split = 0.2,\n","    rotation_range=5,\n","    # width_shift_range=0.2,\n","    # height_shift_range=0.2,\n","    # shear_range=0.2,\n","    # zoom_range=0.2,\n","    horizontal_flip=True,\n","    vertical_flip=True,\n","    # fill_mode='nearest'\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1851,"status":"ok","timestamp":1702515950475,"user":{"displayName":"JiaJun Dai","userId":"04536053890545897660"},"user_tz":480},"id":"ZO4CVrrkxOon","outputId":"15567928-8cde-47a9-d5a4-81e3cad74174"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 24954 images belonging to 7 classes.\n"]}],"source":["train_dataset  = image_generator.flow_from_directory(directory = input_data_path,\n","                                                  target_size = (48,48),\n","                                                  color_mode='rgb',\n","                                                  class_mode = 'categorical',\n","                                                  subset = 'training',\n","                                                  batch_size = 64,\n","                                                  shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1702515950476,"user":{"displayName":"JiaJun Dai","userId":"04536053890545897660"},"user_tz":480},"id":"9Dy1QAZXakDo","outputId":"03d62737-a86b-41a4-c68d-332e76635a1d"},"outputs":[{"name":"stdout","output_type":"stream","text":["(48, 48, 3)\n","(64, 48, 48, 3)\n","(64, 7)\n"]}],"source":["print(train_dataset.image_shape)\n","# Access the next batch of data\n","x_batch, y_batch = train_dataset.next()\n","print(x_batch.shape)\n","print(y_batch.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3831,"status":"ok","timestamp":1702515954303,"user":{"displayName":"JiaJun Dai","userId":"04536053890545897660"},"user_tz":480},"id":"ehWfe7b2FPL2","outputId":"77f6f61a-67d9-418b-aac4-546f00add1f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 6235 images belonging to 7 classes.\n"]}],"source":["validation_dataset = image_generator.flow_from_directory(directory = input_data_path,\n","                                                  target_size = (48,48),\n","                                                  color_mode='rgb',\n","                                                  class_mode = 'categorical',\n","                                                  subset = 'validation',\n","                                                  batch_size = 64)"]},{"cell_type":"markdown","metadata":{"id":"re8M6uG3FVPi"},"source":["Retriving pretrained VGG16 model from tensorflows keras"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JOEMQVXZFTT6"},"outputs":[],"source":["base_model = tf.keras.applications.VGG16(input_shape=(48,48,3),include_top=False,weights=\"imagenet\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jj8g7oA-zh50"},"outputs":[],"source":["# Freezing Layers\n","\n","for layer in base_model.layers[:-4]:\n","    layer.trainable=False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wAkv-oILziaI"},"outputs":[],"source":["# Building Model\n","\n","model=Sequential()\n","model.add(base_model)\n","model.add(Dropout(0.5))\n","model.add(Flatten())\n","model.add(BatchNormalization())\n","model.add(Dense(32,kernel_initializer='he_uniform'))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(32,kernel_initializer='he_uniform'))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(32,kernel_initializer='he_uniform'))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(Dense(7,activation='softmax'))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1702515954684,"user":{"displayName":"JiaJun Dai","userId":"04536053890545897660"},"user_tz":480},"id":"-5RII96qFqY6","outputId":"01a3df42-0510-4b29-bde9-92b58b72b906"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," vgg16 (Functional)          (None, 1, 1, 512)         14714688  \n","                                                                 \n"," dropout_3 (Dropout)         (None, 1, 1, 512)         0         \n","                                                                 \n"," flatten_3 (Flatten)         (None, 512)               0         \n","                                                                 \n"," batch_normalization_4 (Bat  (None, 512)               2048      \n"," chNormalization)                                                \n","                                                                 \n"," dense_11 (Dense)            (None, 32)                16416     \n","                                                                 \n"," batch_normalization_5 (Bat  (None, 32)                128       \n"," chNormalization)                                                \n","                                                                 \n"," activation_3 (Activation)   (None, 32)                0         \n","                                                                 \n"," dropout_4 (Dropout)         (None, 32)                0         \n","                                                                 \n"," dense_12 (Dense)            (None, 32)                1056      \n","                                                                 \n"," batch_normalization_6 (Bat  (None, 32)                128       \n"," chNormalization)                                                \n","                                                                 \n"," activation_4 (Activation)   (None, 32)                0         \n","                                                                 \n"," dropout_5 (Dropout)         (None, 32)                0         \n","                                                                 \n"," dense_13 (Dense)            (None, 32)                1056      \n","                                                                 \n"," batch_normalization_7 (Bat  (None, 32)                128       \n"," chNormalization)                                                \n","                                                                 \n"," activation_5 (Activation)   (None, 32)                0         \n","                                                                 \n"," dense_14 (Dense)            (None, 7)                 231       \n","                                                                 \n","=================================================================\n","Total params: 14735879 (56.21 MB)\n","Trainable params: 7099399 (27.08 MB)\n","Non-trainable params: 7636480 (29.13 MB)\n","_________________________________________________________________\n"]}],"source":["# Model Summary\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eh3XdTT-HeRc"},"outputs":[],"source":["from tensorflow.keras.callbacks import LearningRateScheduler\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","\n","# taken from old keras source code\n","def f1_score(y_true, y_pred):\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n","    return f1_val\n","\n","# Learning rate scheduler\n","def scheduler(epoch, lr):\n","    if epoch < 5:\n","        return lr\n","    else:\n","        return lr * tf.math.exp(-0.1)\n","\n","# Set up LearningRateScheduler\n","lr_scheduler = LearningRateScheduler(scheduler, verbose=1)\n","# lrd = ReduceLROnPlateau(monitor = 'val_loss',patience = 20,verbose = 1,factor = 0.50, min_lr = 1e-10)\n","checkpoint_filepath = '/content/drive/Shareddrives/CMPE 258 - Deep Learning Project/Software Programs/VGG16/VGG16_weights_epoch_{epoch:02d}_val_loss_{val_loss:.4f}.h5'\n","# Set up ModelCheckpoint\n","checkpoint = ModelCheckpoint(\n","    checkpoint_filepath,\n","    monitor='val_loss',            # Monitor the validation loss\n","    verbose=1,                     # Logging level\n","    save_best_only=True,           # Save only the best model\n","    save_weights_only=False,       # If True, only weights are saved\n","    mode='auto',                   # Auto mode means the direction is inferred\n","    save_freq='epoch'              # Save after each epoch\n",")\n","\n","# Set up EarlyStopping\n","early_stopping = EarlyStopping(\n","    monitor='val_loss',  # Monitor validation loss\n","    patience=3,         # Number of epochs with no improvement after which training will be stopped\n","    verbose=1,\n","    restore_best_weights=True  # Restores model weights from the epoch with the best value of the monitored quantity\n",")\n","\n","METRICS = [\n","      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n","      tf.keras.metrics.Precision(name='precision'),\n","      tf.keras.metrics.Recall(name='recall'),\n","      tf.keras.metrics.AUC(name='auc'),\n","      f1_score,\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GtNmTUTvHiGU"},"outputs":[],"source":["model.compile(optimizer='Adam', loss='categorical_crossentropy',metrics=METRICS)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"kXPEzNtEHtGN","outputId":"7fbc6364-91c9-4362-c4d3-076612b0e59a"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n","Epoch 1/20\n"," 62/390 [===>..........................] - ETA: 38s - loss: 2.0973 - accuracy: 0.8506 - precision: 0.2058 - recall: 0.0161 - auc: 0.5481 - f1_score: 0.0295"]}],"source":["history=model.fit(train_dataset,validation_data=validation_dataset,epochs = 20,batch_size=64,callbacks=[lr_scheduler, early_stopping, checkpoint])"]},{"cell_type":"markdown","metadata":{"id":"szWpBD9n-_fw"},"source":["Evaluate trained VGG16 model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W4pImDSx4n9S"},"outputs":[],"source":["!pip install mtcnn\n","import os\n","import cv2\n","import numpy as np\n","from mtcnn import MTCNN\n","\n","data_path = \"/content/drive/Shareddrives/CMPE 258 - Deep Learning Project/Software Programs/Datasets/test\"\n","INPUT_SIZE = (48, 48)\n","classes = [\"surprise\", \"sad\", \"neutral\", \"happy\", \"fear\", \"disgust\", \"angry\"]\n","detector = MTCNN()\n","\n","face_images = []\n","actual_labels = []\n","\n","for emotion_folder in os.listdir(data_path):\n","    emotion_path = os.path.join(data_path, emotion_folder)\n","    if os.path.isdir(emotion_path):\n","       # counter = 0  # Initialize a counter for each folder\n","        for img_file in os.listdir(emotion_path):\n","           # if counter < 5:  # Process only 5 images per folder\n","                img_path = os.path.join(emotion_path, img_file)\n","                image = cv2.imread(img_path)\n","                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","                faces = detector.detect_faces(image_rgb)\n","\n","                for face in faces:\n","                    x, y, width, height = face['box']\n","                    face_image = image_rgb[y:y+height, x:x+width]\n","                    resized_face = cv2.resize(face_image, INPUT_SIZE)\n","                    face_images.append(resized_face)\n","                    actual_labels.append(emotion_folder)  # Folder name as the actual label\n","\n","                #counter += 1  # Increment the counter\n","          #  else:\n","               # break  # Move to the next folder\n","\n","# Convert lists to numpy arrays\n","face_images = np.array(face_images)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6hfbaHRspKY_"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","def smooth_curve(points, factor=0.8):\n","    smoothed_points = []\n","    for point in points:\n","        if smoothed_points:\n","            previous = smoothed_points[-1]\n","            smoothed_points.append(previous * factor + point * (1 - factor))\n","        else:\n","            smoothed_points.append(point)\n","    return smoothed_points\n","\n","\n","acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","print(acc)\n","print(val_acc)\n","print(loss)\n","print(val_loss)\n","\n","smooth_acc = smooth_curve(acc)\n","smooth_val_acc = smooth_curve(val_acc)\n","smooth_loss = smooth_curve(loss)\n","smooth_val_loss = smooth_curve(val_loss)\n","\n","epochs = range(1, len(acc) + 1)\n","\n","# Plot training & validation accuracy\n","plt.figure(figsize=(12, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs, smooth_acc, 'b', label='Smoothed Training accuracy')\n","plt.plot(epochs, smooth_val_acc, 'r', label='Smoothed Validation accuracy')\n","plt.title('Training and Validation Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","\n","# Plot training & validation loss\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs, smooth_loss, 'b', label='Smoothed Training loss')\n","plt.plot(epochs, smooth_val_loss, 'r', label='Smoothed Validation loss')\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0eCaWa6vMU2V"},"outputs":[],"source":["model.save(\"/content/drive/Shareddrives/CMPE 258 - Deep Learning Project/Software Programs/model_data/VGG16ft.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hKPuRYSQMj7e"},"outputs":[],"source":["custom_objects = {'f1_score': f1_score}\n","load_model_vgg16ft=tf.keras.models.load_model(\"/content/drive/Shareddrives/CMPE 258 - Deep Learning Project/Software Programs/model_data/VGG16ft.h5\", custom_objects=custom_objects)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1CVLfD7z0qhK"},"outputs":[],"source":["classes = [\"surprise\", \"sad\", \"neutral\", \"happy\", \"fear\", \"disgust\", \"angry\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f44NMY-UF5iO"},"outputs":[],"source":["predictions = load_model_vgg16ft.predict(face_images)\n","predictions = predictions.reshape(predictions.shape[0], -1)\n","print(face_images[0].shape)\n","print(predictions.shape)\n","print(np.argmax(predictions[0]))\n","# print(classes[np.argmax(predictions[2])])\n","# Convert predictions to labels\n","predicted_labels = [classes[np.argmax(pred)] for pred in predictions]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UzS4wVTQ6xJz"},"outputs":[],"source":["from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Convert actual string labels to indices\n","actual_indices = [classes.index(label) for label in actual_labels]\n","\n","# Convert predicted string labels to indices\n","predicted_indices = [classes.index(label) for label in predicted_labels]\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(actual_indices, predicted_indices)\n","print(f\"Accuracy: {accuracy * 100:.2f}%\")\n","\n","# Generate confusion matrix\n","cm = confusion_matrix(actual_indices, predicted_indices)\n","\n","# Generate and print classification report\n","report = classification_report(actual_indices, predicted_indices, target_names=classes)\n","print(report)\n","\n","# Plot confusion matrix\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n","plt.title(f'Confusion Matrix, Accuracy: {accuracy * 100:.2f}%')\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"90SIZRgtXUCw"},"source":["## VGG16 without finetune"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UFti65J2bVcW"},"outputs":[],"source":["import os\n","from math import ceil\n","import cv2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wcvf8sbkXdP1"},"outputs":[],"source":["datadirectory=\"/content/drive/Shareddrives/CMPE 258 - Deep Learning Project/Software Programs/Datasets/train_data_processed\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yC8OC_V6X1uj"},"outputs":[],"source":["classes=[\"surprise\",\"sad\",\"neutral\",\"happy\",\"fear\",\"disgust\",\"angry\"]\n","img_size=48"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rXcIAt2KX4Vh"},"outputs":[],"source":["from logging import exception\n","training_data=[]\n","def create_training_data():\n","  for category in classes:\n","    path=os.path.join(datadirectory, category)\n","    class_num=classes.index(category)\n","    for img in os.listdir(path):\n","      try:\n","        img_array=cv2.imread(os.path.join(path,img))\n","        new_array=cv2.resize(img_array,(img_size,img_size))\n","        training_data.append([new_array,class_num])\n","      except exception as e:\n","        pass\n","  return training_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"os7wQnD2X65C"},"outputs":[],"source":["train_data = create_training_data()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yuYXEigXb8Do"},"outputs":[],"source":["import random\n","random.shuffle(train_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YGW-9pizb_Y-"},"outputs":[],"source":["x=[]\n","y=[]\n","for features,label in train_data:\n","  x.append(features)\n","  y.append(label)\n","# Convert list x to numpy array\n","x=np.array(x).reshape(-1,img_size,img_size,3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dS8HPgYBe-2j"},"outputs":[],"source":["x.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9UhbR35ofBtL"},"outputs":[],"source":["# Convert list y to numpy array\n","y=np.array(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RAbG2I7OfDu6"},"outputs":[],"source":["y.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fGqZQda7fGB9"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Split the data into training and validation sets (80% training, 20% validation)\n","x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, stratify=y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I3rTVoX8fH1h"},"outputs":[],"source":["print(\"Training Data:\")\n","print(\"Features (x_train):\", x_train.shape)\n","print(\"Labels (y_train):\", y_train.shape)\n","\n","print(\"\\nValidation Data:\")\n","print(\"Features (x_val):\", x_val.shape)\n","print(\"Labels (y_val):\", y_val.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0KcFLqwvfPDZ"},"outputs":[],"source":["base_model_2 = tf.keras.applications.VGG16(input_shape=(48,48,3),include_top=False,weights=\"imagenet\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oanfYOGKfWC-"},"outputs":[],"source":["# Freezing Layers\n","\n","for layer in base_model_2.layers[:-4]:\n","    layer.trainable=False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xCKv5Ywwfa42"},"outputs":[],"source":["# Building Model\n","\n","model=Sequential()\n","model.add(base_model_2)\n","model.add(Dropout(0.5))\n","model.add(Flatten())\n","model.add(BatchNormalization())\n","model.add(Dense(32,kernel_initializer='he_uniform'))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(32,kernel_initializer='he_uniform'))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(32,kernel_initializer='he_uniform'))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(Dense(7,activation='softmax'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5DhuVY-oi-MJ"},"outputs":[],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xGxWheSYflTj"},"outputs":[],"source":["# Compile model\n","model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H3v7wdyjfrGM"},"outputs":[],"source":["EPOCHS = 20\n","history = model.fit(\n","    x_train, y_train,\n","    validation_data=(x_val, y_val),\n","    epochs=EPOCHS,\n","    batch_size=32,\n","    callbacks=[checkpoint, early_stopping, lr_scheduler]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zsqM3BjWoqtH"},"outputs":[],"source":["def smooth_curve(points, factor=0.8):\n","    smoothed_points = []\n","    for point in points:\n","        if smoothed_points:\n","            previous = smoothed_points[-1]\n","            smoothed_points.append(previous * factor + point * (1 - factor))\n","        else:\n","            smoothed_points.append(point)\n","    return smoothed_points\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1S97p-qOorYp"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","print(acc)\n","print(val_acc)\n","print(loss)\n","print(val_loss)\n","\n","smooth_acc = smooth_curve(acc)\n","smooth_val_acc = smooth_curve(val_acc)\n","smooth_loss = smooth_curve(loss)\n","smooth_val_loss = smooth_curve(val_loss)\n","\n","epochs = range(1, len(acc) + 1)\n","\n","# Plot training & validation accuracy\n","plt.figure(figsize=(12, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs, smooth_acc, 'b', label='Smoothed Training accuracy')\n","plt.plot(epochs, smooth_val_acc, 'r', label='Smoothed Validation accuracy')\n","plt.title('Training and Validation Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","\n","# Plot training & validation loss\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs, smooth_loss, 'b', label='Smoothed Training loss')\n","plt.plot(epochs, smooth_val_loss, 'r', label='Smoothed Validation loss')\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bMhz1A6UyGn3"},"outputs":[],"source":["predictions = model.predict(face_images)\n","\n","# Convert predictions to labels\n","predicted_labels = [classes[np.argmax(pred)] for pred in predictions]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9qAIpfs5yTDO"},"outputs":[],"source":["from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Convert actual string labels to indices\n","actual_indices = [classes.index(label) for label in actual_labels]\n","\n","# Convert predicted string labels to indices\n","predicted_indices = [classes.index(label) for label in predicted_labels]\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(actual_indices, predicted_indices)\n","print(f\"Accuracy: {accuracy * 100:.2f}%\")\n","\n","# Generate confusion matrix\n","cm = confusion_matrix(actual_indices, predicted_indices)\n","\n","# Generate and print classification report\n","report = classification_report(actual_indices, predicted_indices, target_names=classes)\n","print(report)\n","\n","# Plot confusion matrix\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n","plt.title('Confusion Matrix')\n","plt.xlabel('Predicted Labels')\n","plt.ylabel('True Labels')\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}